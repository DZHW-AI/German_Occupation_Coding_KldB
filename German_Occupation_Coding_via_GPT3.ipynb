{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GqseRalJZNBd"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wZLnlgwIBcE8"
      },
      "outputs": [],
      "source": [
        "import numpy as np \n",
        "import pandas as pd \n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import pickle\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
        "import re\n",
        "import string\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "from collections import Counter\n",
        "import seaborn as sns\n",
        "from io import StringIO\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_selection import chi2\n",
        "from IPython.display import display\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import balanced_accuracy_score\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import top_k_accuracy_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "from sklearn.metrics import mean_squared_error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xvDKzDcmB9BC"
      },
      "outputs": [],
      "source": [
        "#@title Record Each Cell's Execution Time\n",
        "!pip install ipython-autotime\n",
        "\n",
        "%load_ext autotime"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading data"
      ],
      "metadata": {
        "id": "yFNvxSxCUQaO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5_xL8Kbs6Nqb"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv('dataset.csv',encoding=\"utf-8\") # utf-8 because of the german text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JRIfhLWSCQs3"
      },
      "source": [
        "# Explore the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9GVAT_kZCNA9"
      },
      "outputs": [],
      "source": [
        "data.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FQwDfjWPCS8X"
      },
      "outputs": [],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nPCbg7p0CUZP"
      },
      "outputs": [],
      "source": [
        "data.describe()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def kldb_devision(_data, _start_digit,_end_digit):\n",
        "  _data['col'] = _data['kldb'].astype(str)\n",
        "  _data['kldb_new'] = _data['col'].str[_start_digit:_end_digit]\n"
      ],
      "metadata": {
        "id": "pgTOTnGvUi53"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KXOOoXWhD7GH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5984ff6-8557-4fd9-91a5-6b9481b7bd3c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 610 µs (started: 2022-09-02 08:14:44 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# data['ctotal'] = data.cberuf.astype(str).str.cat(data.chelp1.astype(str), sep=', ')\n",
        "# data['ctotal1'] = data.ctotal.astype(str).str.cat(data.chelp2.astype(str), sep=', ')\n",
        "# data['ctotal2'] = data.ctotal1.astype(str).str.cat(data.chelp3.astype(str), sep=', ')\n",
        "# data['ctotal3'] = data.ctotal2.astype(str).str.cat(data.chelp4.astype(str), sep=', ')\n",
        "# # #data['ctotal4'] = data.ctotal3.astype(str).str.cat(data.chelp5.astype(str), sep=', ')\n",
        "# # # df['cberuf_cfunktion'] = df.cberuf_cfunktion_cfunktion.astype(str).str.cat(df.cfunkti2.astype(str), sep=', ')\n",
        "# # # df.cberuf_cfunktion[0]\n",
        "# data.cberuf = data.ctotal3\n",
        "# data = data.drop(columns=\"ctotal\")\n",
        "# data = data.drop(columns=\"ctotal1\")\n",
        "# data = data.drop(columns=\"ctotal2\")\n",
        "# data = data.drop(columns=\"ctotal3\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n4_KPpsX7FkY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c922553f-ed3c-4623-c983-dceae8b9a440"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 1.28 ms (started: 2022-09-02 08:14:44 +00:00)\n"
          ]
        }
      ],
      "source": [
        "#data.columns=['id'\t,'text'\t,'category','p_lfdnr',\t'col','col1'\t,'chelp5','chelp1',\t'chelp2',\t'chelp3',\t'chelp4','']\n",
        "data.columns=['id','id'\t,'text'\t,'category','p_lfdnr',\t'col','col1'\t,'chelp5','chelp1',\t'chelp2',\t'chelp3',\t'chelp4','ctotal',\t'ctotal1',\t'ctotal2',\t'ctotal3',\t'ctotal4']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "faxqc6SG_cOU"
      },
      "source": [
        " Data Analysis\n",
        "  * The number of reviews of each category:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CvDx3Xe9_hnU"
      },
      "outputs": [],
      "source": [
        "data.category.value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQi0mLWOCdgN"
      },
      "source": [
        " Adding Category Id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yc6pgsbUCnLj"
      },
      "outputs": [],
      "source": [
        "data[\"category\"] = data[\"category\"].astype('category') #By converting an existing Series or column to a category dtype\n",
        "data.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iIcycvniCopf"
      },
      "outputs": [],
      "source": [
        "data[\"category_id\"] = data[\"category\"].cat.codes\n",
        "data.tail()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9LpqhPhTCqVc"
      },
      "outputs": [],
      "source": [
        "data.dtypes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_EcQR1yCwzU"
      },
      "source": [
        "##Dictionaries for future use"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a4f8ahk9CsSR"
      },
      "outputs": [],
      "source": [
        "id_to_category = pd.Series(data.category.values,index=data.category_id).to_dict()\n",
        "id_to_category"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gIpM-ChlCtdJ"
      },
      "outputs": [],
      "source": [
        "category_to_id= {v:k for k,v in id_to_category.items()}\n",
        "category_to_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tRhvio4FC4De"
      },
      "outputs": [],
      "source": [
        "number_of_categories = len(category_to_id)\n",
        "number_of_categories"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mtMmzrMwDGoC"
      },
      "source": [
        " Some necessary variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3VZgFvQzC__J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd3d3622-d1da-4c4b-c75e-8e4bfd693717"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 619 µs (started: 2022-09-02 08:15:16 +00:00)\n"
          ]
        }
      ],
      "source": [
        "#limit the number of samples to be used in code runs\n",
        "data_size= 427230"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XLFiy-nMDF8V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25048d2a-e373-4184-f462-c2951577d779"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 479 µs (started: 2022-09-02 08:15:16 +00:00)\n"
          ]
        }
      ],
      "source": [
        "vocab_size = 90000  # Only consider the top 90k words\n",
        "maxlen = 260  # Max sequence size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zVdcILoqDB_k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "382c178c-e3e0-4a0a-d8d8-1d1571c098d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 7.67 ms (started: 2022-09-02 08:15:16 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# save features and targets from the 'data' (raw data)\n",
        "features, targets = data['text'], data['category_id']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NeblYWT9AtdA"
      },
      "source": [
        "Spliting the data into train and test sets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7YaGx7FAv4i"
      },
      "source": [
        "* We should split the data before the preprocessing. \n",
        "\n",
        "* In order to avoid data leaks, some preprocesses should not be applied on the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ddrweMRqGbh"
      },
      "outputs": [],
      "source": [
        "train_features, test_features, train_targets, test_targets = train_test_split(\n",
        "        features, targets,\n",
        "        train_size=0.9,\n",
        "        test_size=0.1,\n",
        "        # random but same for all run, also accurancy depends on the\n",
        "        # selection of data e.g. if we put 10 then accuracy will be 1.0\n",
        "        # in this example\n",
        "        random_state=46,\n",
        "        # keep same proportion of 'target' in test and target data\n",
        "        #stratify=targets\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPd5aAJmDtpn"
      },
      "source": [
        " Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yc9_90UfDvKY"
      },
      "outputs": [],
      "source": [
        "def causal_attention_mask(batch_size, n_dest, n_src, dtype):\n",
        "    \"\"\"\n",
        "    Mask the upper half of the dot product matrix in self attention.\n",
        "    This prevents flow of information from future tokens to current token.\n",
        "    1's in the lower triangle, counting from the lower right corner.\n",
        "    \"\"\"\n",
        "    i = tf.range(n_dest)[:, None]\n",
        "    j = tf.range(n_src)\n",
        "    m = i >= j - n_src + n_dest\n",
        "    mask = tf.cast(m, dtype)\n",
        "    mask = tf.reshape(mask, [1, n_dest, n_src])\n",
        "    mult = tf.concat(\n",
        "        [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)], 0\n",
        "    )\n",
        "    return tf.tile(mask, mult)\n",
        "\n",
        "\n",
        "class TransformerBlock(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1, **kwargs):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.embed_dim=embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.ff_dim =ff_dim\n",
        "        self.att = layers.MultiHeadAttention(num_heads, embed_dim)\n",
        "        self.ffn = keras.Sequential(\n",
        "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = layers.Dropout(rate)\n",
        "        self.dropout2 = layers.Dropout(rate)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        input_shape = tf.shape(inputs)\n",
        "        batch_size = input_shape[0]\n",
        "        seq_len = input_shape[1]\n",
        "        # for masked-self attention add the mask:\n",
        "        # causal_mask = causal_attention_mask(batch_size, seq_len, seq_len, tf.bool)\n",
        "        # attention_output = self.att(inputs, inputs,attention_mask=causal_mask)\n",
        "        \n",
        "        attention_output = self.att(inputs, inputs) \n",
        "        \n",
        "        attention_output = self.dropout1(attention_output)\n",
        "        out1 = self.layernorm1(inputs + attention_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output)\n",
        "        return self.layernorm2(out1 + ffn_output)\n",
        "    \n",
        "    # https://newbedev.com/saving-keras-models-with-custom-layers\n",
        "    def get_config(self):\n",
        "        config = super(TransformerBlock, self).get_config()\n",
        "        config.update({\n",
        "            'att': self.att,\n",
        "            'ffn': self.ffn,\n",
        "            #'layernorm1': self.layernorm1,\n",
        "            #'layernorm2':self.layernorm2,\n",
        "            'dropout1':self.dropout1,\n",
        "            'dropout2':self.dropout2,\n",
        "            'embed_dim': self.embed_dim,\n",
        "            'num_heads':self.num_heads,\n",
        "            'ff_dim':self.ff_dim \n",
        "\n",
        "        })\n",
        "        return config\n",
        "        #tf.keras.models.save_model(model, 'model.h5')\n",
        "        #new_model = tf.keras.models.load_model('model.h5', custom_objects={'CustomLayer': CustomLayer})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JsmbAa7mDxHy"
      },
      "outputs": [],
      "source": [
        "class TokenPositionEmbedding(layers.Layer):\n",
        "    def __init__(self, maxlen, vocab_size,  embed_dim, **kwargs):\n",
        "        super(TokenPositionEmbedding, self).__init__()\n",
        "        self.maxlen = maxlen\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_dim = embed_dim\n",
        "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
        "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
        "\n",
        "    def call(self, X):\n",
        "        maxlen = tf.shape(X)[-1]\n",
        "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
        "        positions = self.pos_emb(positions)\n",
        "        X = self.token_emb(X)\n",
        "        return X + positions \n",
        "        \n",
        "    # https://newbedev.com/saving-keras-models-with-custom-layers\n",
        "    def get_config(self):\n",
        "        config = super(TokenPositionEmbedding, self).get_config()\n",
        "        config.update({\n",
        "            'token_emb': self.token_emb,\n",
        "            'pos_emb': self.pos_emb,\n",
        "            'maxlen': self.maxlen,\n",
        "            'vocab_size': self.vocab_size,\n",
        "            'embed_dim': self.embed_dim\n",
        "        })\n",
        "        return config\n",
        "        #tf.keras.models.save_model(model, 'model.h5')\n",
        "        #new_model = tf.keras.models.load_model('model.h5', custom_objects={'CustomLayer': CustomLayer})\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g5pVNUymDy4q"
      },
      "outputs": [],
      "source": [
        "embed_dim = 256  # Embedding size for each token\n",
        "num_heads = 2  # Number of attention heads\n",
        "feed_forward_dim = 256  # Hidden layer size in feed forward network inside transformer\n",
        "\n",
        "\n",
        "def create_model():\n",
        "    inputs_tokens = layers.Input(shape=(maxlen,), dtype=tf.int32)\n",
        "    embedding_layer = TokenPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
        "    x = embedding_layer(inputs_tokens)\n",
        "    transformer_block1 = TransformerBlock(embed_dim, num_heads, feed_forward_dim)\n",
        "    transformer_block2 = TransformerBlock(embed_dim, num_heads, feed_forward_dim)\n",
        "    x = transformer_block1(x)\n",
        "    x = transformer_block2(x)\n",
        "    x = layers.Flatten()(x)\n",
        "    outputs = layers.Dense(number_of_categories)(x)\n",
        "    model = keras.Model(inputs=inputs_tokens, outputs=outputs)\n",
        "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "    metric_fn  = tf.keras.metrics.SparseCategoricalAccuracy()\n",
        "    model.compile(optimizer=\"adam\", loss=loss_fn, metrics=metric_fn)  \n",
        "    \n",
        "    return model\n",
        "my_model=create_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZFC3ZbWvD1mK"
      },
      "outputs": [],
      "source": [
        "my_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CCja2KoCD3Fd"
      },
      "outputs": [],
      "source": [
        "tf.keras.utils.plot_model(my_model,show_shapes=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yRy2E-DhD4sw"
      },
      "outputs": [],
      "source": [
        "%cd /content/gdrive/My Drive/Colab Notebooks/models\n",
        "%pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7FUiGv5hD6D8"
      },
      "outputs": [],
      "source": [
        "checkpoint_filepath = './checkpoint'\n",
        "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath,\n",
        "    save_weights_only=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BD-ZGPCTD8Ms"
      },
      "outputs": [],
      "source": [
        "X,y = train_features,train_targets\n",
        "\n",
        "balanced_accuracy_scores = []\n",
        "matthews_corrcoef_scores = []\n",
        "f1_scores = []\n",
        "conf_matrix_list_of_arrays = []\n",
        "\n",
        "# prepare cross validation\n",
        "n=5\n",
        "seed=1\n",
        "k_fold = KFold(n_splits=n, random_state=seed, shuffle=True)\n",
        "\n",
        "for train_index, test_index in k_fold.split(X,y):\n",
        "  X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
        "  y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
        "\n",
        "  X_train_dtm = X_train\n",
        "  X_test_dtm = X_test\n",
        "  history = my_model.fit(X_train_dtm, y_train, verbose=1, epochs=1, callbacks=[model_checkpoint_callback])\n",
        "  y_pred_class = my_model.predict(X_test_dtm)\n",
        "\n",
        "  y_pred_class = np.argmax(y_pred_class,axis=1)\n",
        "  \n",
        "  conf_matrix = confusion_matrix(y_test, y_pred_class)\n",
        "  conf_matrix_list_of_arrays.append(conf_matrix)\n",
        "  \n",
        "  balanced_accuracy_scores.append(balanced_accuracy_score(y_test, y_pred_class))\n",
        "  matthews_corrcoef_scores.append(matthews_corrcoef(y_test, y_pred_class))\n",
        "  f1_scores.append(f1_score(y_test, y_pred_class, average='weighted'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6mH6mpIPOXvP"
      },
      "source": [
        "Average sparse_categorical_accuracy: 0.9643"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XFPR3PThyHvC"
      },
      "outputs": [],
      "source": [
        "mean_of_conf_matrix_arrays = np.mean(conf_matrix_list_of_arrays, axis=0)\n",
        "print('Mean of conf_matrix: ', mean_of_conf_matrix_arrays)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HcyGIkIH9bqZ"
      },
      "outputs": [],
      "source": [
        "conf_mat = mean_of_conf_matrix_arrays.astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aPGoOqhKP2IJ"
      },
      "outputs": [],
      "source": [
        "category_id_df = data[['category', 'category_id']].drop_duplicates()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3P-_eylb7_Yk"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(8,8))\n",
        "sns.heatmap(conf_mat, annot=True, cmap=\"Blues\", fmt='d',\n",
        "            xticklabels=category_id_df.category.values, \n",
        "            yticklabels=category_id_df.category.values)\n",
        "plt.ylabel('Actual')\n",
        "plt.xlabel('Predicted')\n",
        "plt.title(\"CONFUSION MATRIX \\n\", size=16);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLqVBl5VLTI3"
      },
      "source": [
        "Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e9vseM-HY6IA"
      },
      "outputs": [],
      "source": [
        "balanced_accuracy_scores = np.array(balanced_accuracy_scores)\n",
        "print('Mean of balanced_accuracy_scores: ', np.mean(balanced_accuracy_scores, axis=0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d9Vq_tdhY6h8"
      },
      "outputs": [],
      "source": [
        "matthews_corrcoef_scores = np.array(matthews_corrcoef_scores)\n",
        "print('Mean of matthews_corrcoef_scores: ', np.mean(matthews_corrcoef_scores, axis=0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q52Whq2tomXJ"
      },
      "outputs": [],
      "source": [
        "f1_scores = np.array(f1_scores)\n",
        "print('Mean of f1_scores: ', np.mean(f1_scores, axis=0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eaprE9CYqtaX"
      },
      "source": [
        "**Test the Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7OneytVGG87v"
      },
      "outputs": [],
      "source": [
        "test_features_vc = test_features\n",
        "y_pred_test = my_model.predict(test_features_vc)\n",
        "y_pred_test = np.argmax(y_pred_test,axis=1)\n",
        " \n",
        "balanced_accuracy_score_test = balanced_accuracy_score(test_targets, y_pred_test)\n",
        "print('balanced_accuracy_score_test :',balanced_accuracy_score_test)\n",
        "matthews_corrcoef_score_test = matthews_corrcoef(test_targets, y_pred_test)\n",
        "print('matthews_corrcoef_score_test :',matthews_corrcoef_score_test)\n",
        "f1_score_test = f1_score(test_targets, y_pred_test, average='weighted')\n",
        "print('f1_score_test :',f1_score_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5rYybVp8BydW"
      },
      "outputs": [],
      "source": [
        "new_review = \"\"\n",
        "predictions=my_model.predict([new_review])\n",
        "for pred in predictions:\n",
        "  print(id_to_category[np.argmax(pred)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iWQAKPDV2Cdj"
      },
      "outputs": [],
      "source": [
        "test = pd.read_csv('TEST.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SXzwtbjY4LFj"
      },
      "outputs": [],
      "source": [
        "# test['ctotal'] = test.cberuf.astype(str).str.cat(test.chelp2.astype(str), sep=', ')\n",
        "# # #data['ctotal1'] = data.ctotal.astype(str).str.cat(data.chelp2.astype(str), sep=', ')\n",
        "# # # data['ctotal2'] = data.ctotal1.astype(str).str.cat(data.chelp3.astype(str), sep=', ')\n",
        "# # # data['ctotal3'] = data.ctotal2.astype(str).str.cat(data.chelp4.astype(str), sep=', ')\n",
        "# # # #data['ctotal4'] = data.ctotal3.astype(str).str.cat(data.chelp5.astype(str), sep=', ')\n",
        "# # # # df['cberuf_cfunktion'] = df.cberuf_cfunktion_cfunktion.astype(str).str.cat(df.cfunkti2.astype(str), sep=', ')\n",
        "# # # # df.cberuf_cfunktion[0]\n",
        "# test.cberuf = test.ctotal\n",
        "# test = test.drop(columns=\"ctotal\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hdDdpD1X_I_l"
      },
      "outputs": [],
      "source": [
        "#test = pd.read_csv('TEST.csv')\n",
        "jobs = test.cberuf\n",
        "#last_kldb=test.predict\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dfMQPO2IAJg7"
      },
      "outputs": [],
      "source": [
        "# test['col'] = test['all-kldb'].astype(str)\n",
        "\n",
        "# # make the new columns using string indexing\n",
        "# test['kldb0'] = test['col'].str[4:5]\n",
        "# #test.kldb=test.kldb0\n",
        "# true = []\n",
        "# true = test.kldb0\n",
        "# all = []\n",
        "# all=test.col"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bAA9MNQK0w_Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6fef9df9-82c4-4a8c-8bb6-34ddd5ed1755"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 972 µs (started: 2022-09-02 08:30:49 +00:00)\n"
          ]
        }
      ],
      "source": [
        "#true=[]\n",
        "true = test.kldb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RTtzrck7CYKM"
      },
      "outputs": [],
      "source": [
        "\n",
        "predictions=my_model.predict(jobs)\n",
        "result = []\n",
        "arr = []\n",
        "#lastKldb=[]\n",
        "#lastKldb =test.last_predict\n",
        "i=0\n",
        "for  i in range(len(predictions)):\n",
        "  arr.append(id_to_category[np.argmax(predictions[i])])\n",
        "  arr.append(jobs[i])\n",
        "  arr.append(true[i])\n",
        "  #arr.append(all[i])\n",
        "  #arr.append(last_kldb[i])\n",
        "  result.append(arr)\n",
        "  arr=[]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GQwFWQw5-Yio"
      },
      "outputs": [],
      "source": [
        "result\n",
        "dp = pd.DataFrame(result)\n",
        "dp.columns=['predict','occupation','true']\n",
        "dp.to_csv('result.csv')\n",
        "from google.colab import files\n",
        "files.download(\"result.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dp['x'] = dp.last_kldb.astype(str).str.cat(dp.predict.astype(str), sep='')\n",
        "dp.last_kldb=dp.x"
      ],
      "metadata": {
        "id": "7wrAA1Hcuvcd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "save the model"
      ],
      "metadata": {
        "id": "N9GE6fj_ZfBV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gO1H2PDTcg4a"
      },
      "outputs": [],
      "source": [
        "tf.keras.models.save_model(my_model, 'MultiClassTextClassifier')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}